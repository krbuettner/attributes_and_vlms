{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ac8ddf",
   "metadata": {},
   "source": [
    "# Text-Region Retrieval (Object-Attribute Queries) - CLIP\n",
    "## EX: Retrieve bounding boxes for query \"red bird\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf463e",
   "metadata": {},
   "source": [
    "## Setup: GPUs, Key Variables, Paths, Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52469a04",
   "metadata": {},
   "source": [
    "### GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eee64287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n"
     ]
    }
   ],
   "source": [
    "# Notebook assumes 1 GPU can be used; If running on server, note which are available \n",
    "\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
    "\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886f9ed1",
   "metadata": {},
   "source": [
    "### Key Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c99278bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loading CLIP \n",
    "MODE_FOR_MODEL = \"custom_trained\"  # \"custom_trained\" (e.g. adjective negs), \"clip_lib\" (def clip), \"default_openai\" (def openclip)\n",
    "BACKBONE = \"ViT-B-32\"              # Script is only tested with ViT-B-32\n",
    "\n",
    "# If we need to preprocess a query file, note \n",
    "MAKE_QUERY_FILE = False \n",
    "MAKE_MODE = \"color_pattern_material\" # attributes of interest separated by underscore\n",
    "NUM_OF_GT_RETRIEVALS_THRESHOLD = 10  # since we eval @k, make sure at least 10 regions exist in dataset \n",
    "\n",
    "# Prompt for CLIP inference\n",
    "PROMPT_TEMPLATE = 'A photo of a {attribute_name} {category_name}.'\n",
    "\n",
    "# Attributes for CLIP evaluation\n",
    "ATTRIBUTE_TYPES_OF_INTEREST = [\"color\", \"material\", \"pattern\"]\n",
    "\n",
    "# We can preprocess crops to save time\n",
    "LOAD_IMG_INFO = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a62e50",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc192f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to model to analyze\n",
    "PATH_TO_MODEL = '/afs/cs.pitt.edu/usr0/krb115/private/clip_attributes/prep_for_git/attributes_and_vlms/data/random_neg_bs64_lr1en6_epoch_5.pt'\n",
    "#PATH_TO_MODEL = '/archive2/kyle/clip_training_exp/pt_6_13_23_order_only_attr_noun/checkpoints/epoch_5.pt'\n",
    "#PATH_TO_MODEL = '/archive2/kyle/clip_training_exp/fix_pt_6_12_baseline_w_filtered_negs/checkpoints/epoch_5.pt'\n",
    "\n",
    "# Path to object-attribute combo file \n",
    "# If this is unavailable, this script will provide option to make file\n",
    "# Note: Refer to section which makes file for note about cleaning required after creation\n",
    "PATH_TO_ATTR_OBJ_COMBOS = \"/afs/cs.pitt.edu/usr0/krb115/private/clip_attributes/prep_for_git/attributes_and_vlms/data/color_pattern_material_cleaned.txt\"\n",
    "\n",
    "# Path to needed datasets \n",
    "PATH_TO_COCO = '/archive2/kyle/datasets/coco/val2017/'\n",
    "PATH_TO_OVAD = \"/archive2/kyle/datasets/ovad/ovad2000.json\"\n",
    "\n",
    "# Path to work area to save temporary files \n",
    "WORK_AREA_PATH = \"/afs/cs.pitt.edu/usr0/krb115/private/clip_attributes/work_area/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf61bd9b",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5198362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define COCO class mapping (this is standard set dict of ids to names) \n",
    "COCO_NUMS_TO_NAMES = {1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane', 6: 'bus', 7: 'train', 8: 'truck', \n",
    "            9: 'boat', 10: 'traffic light', 11: 'fire hydrant', 13: 'stop sign', 14: 'parking meter', \n",
    "            15: 'bench', 16: 'bird', 17: 'cat', 18: 'dog', 19: 'horse', 20: 'sheep', 21: 'cow', 22: 'elephant', \n",
    "            23: 'bear', 24: 'zebra', 25: 'giraffe', 27: 'backpack', 28: 'umbrella', 31: 'handbag', 32: 'tie', 33: 'suitcase',\n",
    "            34: 'frisbee', 35: 'skis', 36: 'snowboard', 37: 'sports ball', 38: 'kite', 39: 'baseball bat', 40: 'baseball glove', 41: 'skateboard', \n",
    "            42: 'surfboard', 43: 'tennis racket', 44: 'bottle', 46: 'wine glass', 47: 'cup', 48: 'fork', 49: 'knife', 50: 'spoon', 51: 'bowl', \n",
    "            52: 'banana', 53: 'apple', 54: 'sandwich', 55: 'orange', 56: 'broccoli', 57: 'carrot', 58: 'hot dog', 59: 'pizza', 60: 'donut', \n",
    "            61: 'cake', 62: 'chair', 63: 'couch', 64: 'potted plant', 65: 'bed', 67: 'dining table', 70: 'toilet', 72: 'tv', 73: 'laptop', \n",
    "            74: 'mouse', 75: 'remote', 76: 'keyboard', 77: 'cell phone', 78: 'microwave', 79: 'oven', 80: 'toaster', 81: 'sink', 82: 'refrigerator', \n",
    "            84: 'book', 85: 'clock', 86: 'vase', 87: 'scissors', 88: 'teddy bear', 89: 'hair drier', 90: 'toothbrush'}\n",
    "\n",
    "# OVAD\n",
    "COLOR_IDS = [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]\n",
    "MATERIAL_IDS = [70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
    "PATTERN_IDS = [87, 88, 89, 90, 91, 92, 93]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27c90a4",
   "metadata": {},
   "source": [
    "### Key Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8fc561d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/cs.pitt.edu/usr0/krb115/miniconda3/envs/open_clip/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Key imports and setup \n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../vision_language_models_are_bows\")         # Add subrepo to Python path \n",
    "import clip\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "from matplotlib.patches import Rectangle\n",
    "from model_zoo import get_model\n",
    "from model_zoo.clip_models import CLIPWrapper\n",
    "import numpy as np\n",
    "from open_clip import create_model_and_transforms             # Need OpenCLIP installed \n",
    "import pickle\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0109b482",
   "metadata": {},
   "source": [
    "## Preparation for Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c976521",
   "metadata": {},
   "source": [
    "### Load Model of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9db0b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model \n",
    "if MODE_FOR_MODEL == \"custom_trained\":\n",
    "    checkpoint = torch.load(PATH_TO_MODEL, map_location=torch.device(DEVICE))\n",
    "    model, _, preprocess = create_model_and_transforms(\n",
    "            BACKBONE, \"openai\", precision='amp', device=DEVICE, jit=False, force_quick_gelu=False, pretrained_image=False)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model = model.eval()        \n",
    "    model = CLIPWrapper(model, DEVICE)\n",
    "# Using default model in CLIP library \n",
    "elif MODE_FOR_MODEL == \"clip_lib\":\n",
    "    if \"ViT\" in BACKBONE:               # clean text format\n",
    "        modified_backbone = BACKBONE[::-1].replace('-', '/', 1)[::-1]\n",
    "    model, preprocess = clip.load(modified_backbone, device=DEVICE)\n",
    "# If using OpenCLIP default model \n",
    "elif MODE_FOR_MODEL == \"default_openai\":\n",
    "    if \"ViT\" in BACKBONE:               # clean text format\n",
    "        modified_backbone = BACKBONE[::-1].replace('-', '/', 1)[::-1]\n",
    "    model, preprocess = get_model(model_name=\"openai-clip:\" + modified_backbone, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dbdb7a",
   "metadata": {},
   "source": [
    "### Load OVAD: Dictionary for ID # to Attribute Name, Image List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b319cb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'cleanliness:clean/neat', 1: 'cleanliness:unclean/dirt/dirty/muddy', 2: 'clothes color:black', 3: 'clothes color:blue', 4: 'clothes color:brown', 5: 'clothes color:gray', 6: 'clothes color:green', 7: 'clothes color:orange', 8: 'clothes color:pink', 9: 'clothes color:red', 10: 'clothes color:tan', 11: 'clothes color:violet', 12: 'clothes color:white', 13: 'clothes color:yellow', 14: 'clothes pattern:dotted/speckled/spotted', 15: 'clothes pattern:floral', 16: 'clothes pattern:lettered', 17: 'clothes pattern:plaid/tartan/checkered', 18: 'clothes pattern:plain', 19: 'clothes pattern:striped/lined/pinstriped', 20: 'clothes pattern:tiled', 21: 'color quantity:multicolored/colorful', 22: 'color quantity:single-colored/unicolored', 23: 'color quantity:two-colored', 24: 'color:black', 25: 'color:blue', 26: 'color:brown', 27: 'color:gray', 28: 'color:green', 29: 'color:orange', 30: 'color:pink', 31: 'color:red', 32: 'color:tan', 33: 'color:violet', 34: 'color:white', 35: 'color:yellow', 36: 'cooked:cooked/baked/warmed', 37: 'cooked:raw/fresh', 38: 'face expression:angry/mad', 39: 'face expression:disgust/frowning', 40: 'face expression:fear/surprise', 41: 'face expression:happy/smiling/laughing/grinning/joyful', 42: 'face expression:neutral/calm/serious', 43: 'face expression:sad/unhappy', 44: 'face expression:sleepy/sleeping', 45: 'gender:female/woman/girl', 46: 'gender:male/man/guy/boy', 47: 'group:group/bunch/collection', 48: 'group:single/one/individual/sole', 49: 'hair color:black', 50: 'hair color:blue', 51: 'hair color:brown', 52: 'hair color:gray', 53: 'hair color:green', 54: 'hair color:orange', 55: 'hair color:pink', 56: 'hair color:red', 57: 'hair color:tan', 58: 'hair color:violet', 59: 'hair color:white', 60: 'hair color:yellow', 61: 'hair length:bald', 62: 'hair length:long', 63: 'hair length:short', 64: 'hair tone:dark', 65: 'hair tone:light/bright', 66: 'hair type:curly/curled', 67: 'hair type:straight', 68: 'length:long', 69: 'length:short', 70: 'material:asphalt/cement/clay/concrete/stucco', 71: 'material:ceramic/brick/porcelain', 72: 'material:glass', 73: 'material:leather', 74: 'material:metal/metallic/aluminum/brass/copper-zinc/iron/stainless steel/steel/silver', 75: 'material:paper/cardboard', 76: 'material:polymers/plastic/rubber/styrofoam/polymer', 77: 'material:stone/granite/cobblestone/gravel/marble/pebbled/rocky/sandy', 78: 'material:textile/cloth/fabric/denim/cotton/jean/silk/plush', 79: 'material:wood/wooden/bamboo/hardwood', 80: 'maturity:adult/old/aged', 81: 'maturity:young/baby', 82: 'optical property:opaque', 83: 'optical property:reflective', 84: 'optical property:transparent/translucent', 85: 'order:messy/disordered/unordered/disorganized/unorganized/cluttered/untidy', 86: 'order:ordered/arranged/organized/tidy', 87: 'pattern:dotted/speckled/spotted', 88: 'pattern:floral', 89: 'pattern:lettered', 90: 'pattern:plaid/tartan/checkered', 91: 'pattern:plain', 92: 'pattern:striped/lined/pinstriped', 93: 'pattern:tiled', 94: 'position:horizontal/lying', 95: 'position:sitting/sit', 96: 'position:vertical/upright/standing', 97: 'size:big/large/giant/huge', 98: 'size:small/little/tiny', 99: 'state:closed', 100: 'state:covered', 101: 'state:cracked', 102: 'state:dilapidated/ruined/broken', 103: 'state:dry', 104: 'state:empty', 105: 'state:folded/bend', 106: 'state:full/whole', 107: 'state:off', 108: 'state:on', 109: 'state:open', 110: 'state:piece/cut', 111: 'state:wet', 112: 'texture:rough', 113: 'texture:smooth/sleek', 114: 'texture:soft/fluffy/furry/hairy', 115: 'tone:dark', 116: 'tone:light/bright'}\n"
     ]
    }
   ],
   "source": [
    "# Make id_to_attribute_dict to know attributes in dataset \n",
    "with open(PATH_TO_OVAD, \"r\") as f:\n",
    "    ovad_data = json.load(f)\n",
    "id_to_attribute_dict = dict()\n",
    "for attr in ovad_data[\"attributes\"]:\n",
    "    id_to_attribute_dict[attr['id']] = attr['name']\n",
    "print(id_to_attribute_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2489482e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Images in OVAD\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "imgs_in_ovad = []\n",
    "for img_meta in ovad_data['images']:\n",
    "    imgs_in_ovad.append(img_meta['id'])\n",
    "print('# of Images in OVAD')\n",
    "print(len(imgs_in_ovad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88ebfc",
   "metadata": {},
   "source": [
    "### If we need to make a query file, do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92f65f79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if MAKE_QUERY_FILE:\n",
    "    \n",
    "    # Get attribute names/ids of interest \n",
    "    names_to_make_file = []\n",
    "    attribute_ids_to_make_file = []\n",
    "    if \"color\" in MAKE_MODE:\n",
    "        names_to_make_file += [\"black\", \"blue\", \"brown\", \"gray\", \"green\", \"orange\", \"pink\", \"red\", \"tan\", \"violet\", \"white\", \"yellow\"]\n",
    "        attribute_ids_to_make_file += [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]\n",
    "    if \"material\" in MAKE_MODE:\n",
    "        names_to_make_file += [\"asphalt/cement/clay/concrete/stucco\", \"ceramic/brick/porcelain\", \"glass\", \"leather\", \"metal/metallic/aluminum/brass/copper-zinc/iron/stainless steel/steel/silver\", \"paper/cardboard\", \"polymers/plastic/rubber/styrofoam/polymer\", \"stone/granite/cobblestone/gravel/marble/pebbled/rocky/sandy\", \"textile/cloth/fabric/denim/cotton/jean/silk/plush\", \"wood/wooden/bamboo/hardwood\"]\n",
    "        attribute_ids_to_make_file += [70, 71, 72, 73, 74, 75, 76, 77, 78, 79]     \n",
    "    if \"pattern\" in MAKE_MODE:\n",
    "        names_to_make_file += [\"dotted/speckled/spotted\", \"floral\", \"lettered\", \"pattern:plaid/tartan/checkered\", \"plain\", \"pattern:striped/lined/pinstriped\", \"tiled\"]\n",
    "        attribute_ids_to_make_file += [87, 88, 89, 90, 91, 92, 93]\n",
    "\n",
    "    # Need to make dictionary counting attributes for each class in COCO - loop through annots \n",
    "    dictionary_coco_cls_to_attr_count = dict()\n",
    "    for i in COCO_NUMS_TO_NAMES:\n",
    "        dictionary_coco_cls_to_attr_count[i] = dict() # 1: dict()\n",
    "        for j in attribute_ids_to_make_file:\n",
    "            dictionary_coco_cls_to_attr_count[i][j] = 0 # 1: 24: \n",
    "    for i, annot in enumerate(ovad_data[\"annotations\"]):\n",
    "        for index in attribute_ids_to_make_file:\n",
    "            if annot['att_vec'][index] == 1:\n",
    "                dictionary_coco_cls_to_attr_count[annot['category_id']][index] += 1\n",
    "\n",
    "    # Make set of queries to use, with greater than 10 examples \n",
    "    attr_and_objs_to_retrieve = []\n",
    "    for coco_cls in dictionary_coco_cls_to_attr_count:\n",
    "        for attr_ind in dictionary_coco_cls_to_attr_count[coco_cls]:\n",
    "            if dictionary_coco_cls_to_attr_count[coco_cls][attr_ind] > NUM_OF_GT_RETRIEVALS_THRESHOLD:\n",
    "                adj_of_use = names_to_make_file[attribute_ids_to_make_file.index(attr_ind)]\n",
    "                attr_and_objs_to_retrieve.append([adj_of_use, COCO_NUMS_TO_NAMES[coco_cls], attr_ind, coco_cls])\n",
    "\n",
    "    # Write as attr index, COCO class index, attr name, COCO name\n",
    "    with open(PATH_TO_ATTR_OBJ_COMBOS, 'w') as f:\n",
    "        for a in attr_and_objs_to_retrieve:\n",
    "            f.write(str(a[2]) + ',' + str(a[3]) + ',' + str(a[0]) + ',' + str(a[1]) + '\\n')\n",
    "            \n",
    "    print('An extra filtering step is needed (externally) to make realistic attribute-object combos and to prune others')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8a35a8",
   "metadata": {},
   "source": [
    "### Load attributes and objects to retrieve and format as \"queries\" list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c87696cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323 combinations\n"
     ]
    }
   ],
   "source": [
    "# First extract lists from file \n",
    "attr_and_objs_to_retrieve = []\n",
    "with open(PATH_TO_ATTR_OBJ_COMBOS, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        attr_and_objs_to_retrieve.append(line.strip().split(','))\n",
    "print(str(len(attr_and_objs_to_retrieve)) + ' combinations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1545d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries\n",
      "['A photo of a black bicycle.', 'A photo of a gray bicycle.', 'A photo of a white bicycle.', 'A photo of a black car.', 'A photo of a blue car.', 'A photo of a gray car.', 'A photo of a red car.', 'A photo of a white car.', 'A photo of a yellow car.', 'A photo of a black motorcycle.', 'A photo of a blue motorcycle.', 'A photo of a gray motorcycle.', 'A photo of a red motorcycle.', 'A photo of a white motorcycle.', 'A photo of a blue airplane.', 'A photo of a gray airplane.', 'A photo of a red airplane.', 'A photo of a white airplane.', 'A photo of a black bus.', 'A photo of a blue bus.', 'A photo of a red bus.', 'A photo of a white bus.', 'A photo of a yellow bus.', 'A photo of a black train.', 'A photo of a blue train.', 'A photo of a gray train.', 'A photo of a red train.', 'A photo of a white train.', 'A photo of a yellow train.', 'A photo of a black truck.', 'A photo of a gray truck.', 'A photo of a red truck.', 'A photo of a white truck.', 'A photo of a black boat.', 'A photo of a blue boat.', 'A photo of a red boat.', 'A photo of a white boat.', 'A photo of a yellow boat.', 'A photo of a black traffic light.', 'A photo of a green traffic light.', 'A photo of a red traffic light.', 'A photo of a yellow traffic light.', 'A photo of a red fire hydrant.', 'A photo of a yellow fire hydrant.', 'A photo of a black stop sign.', 'A photo of a red stop sign.', 'A photo of a white stop sign.', 'A photo of a black parking meter.', 'A photo of a gray parking meter.', 'A photo of a black bench.', 'A photo of a brown bench.', 'A photo of a gray bench.', 'A photo of a white bench.', 'A photo of a black bird.', 'A photo of a brown bird.', 'A photo of a gray bird.', 'A photo of a white bird.', 'A photo of a black cat.', 'A photo of a brown cat.', 'A photo of a tan cat.', 'A photo of a white cat.', 'A photo of a black dog.', 'A photo of a brown dog.', 'A photo of a tan dog.', 'A photo of a white dog.', 'A photo of a black horse.', 'A photo of a brown horse.', 'A photo of a tan horse.', 'A photo of a white horse.', 'A photo of a black sheep.', 'A photo of a brown sheep.', 'A photo of a gray sheep.', 'A photo of a tan sheep.', 'A photo of a white sheep.', 'A photo of a black cow.', 'A photo of a brown cow.', 'A photo of a gray cow.', 'A photo of a white cow.', 'A photo of a brown elephant.', 'A photo of a gray elephant.', 'A photo of a tan elephant.', 'A photo of a black bear.', 'A photo of a brown bear.', 'A photo of a black zebra.', 'A photo of a white zebra.', 'A photo of a brown giraffe.', 'A photo of a tan giraffe.', 'A photo of a black backpack.', 'A photo of a blue backpack.', 'A photo of a gray backpack.', 'A photo of a white backpack.', 'A photo of a black umbrella.', 'A photo of a blue umbrella.', 'A photo of a red umbrella.', 'A photo of a white umbrella.', 'A photo of a black handbag.', 'A photo of a blue handbag.', 'A photo of a brown handbag.', 'A photo of a red handbag.', 'A photo of a white handbag.', 'A photo of a black tie.', 'A photo of a blue tie.', 'A photo of a gray tie.', 'A photo of a red tie.', 'A photo of a white tie.', 'A photo of a black suitcase.', 'A photo of a blue suitcase.', 'A photo of a brown suitcase.', 'A photo of a gray suitcase.', 'A photo of a red suitcase.', 'A photo of a red frisbee.', 'A photo of a white frisbee.', 'A photo of a yellow frisbee.', 'A photo of a black skis.', 'A photo of a white skis.', 'A photo of a black snowboard.', 'A photo of a green sports ball.', 'A photo of a white sports ball.', 'A photo of a yellow sports ball.', 'A photo of a black kite.', 'A photo of a blue kite.', 'A photo of a green kite.', 'A photo of a orange kite.', 'A photo of a red kite.', 'A photo of a white kite.', 'A photo of a yellow kite.', 'A photo of a black baseball bat.', 'A photo of a brown baseball bat.', 'A photo of a tan baseball bat.', 'A photo of a white baseball bat.', 'A photo of a black baseball glove.', 'A photo of a brown baseball glove.', 'A photo of a black skateboard.', 'A photo of a tan skateboard.', 'A photo of a black surfboard.', 'A photo of a blue surfboard.', 'A photo of a tan surfboard.', 'A photo of a white surfboard.', 'A photo of a yellow surfboard.', 'A photo of a black tennis racket.', 'A photo of a white tennis racket.', 'A photo of a black bottle.', 'A photo of a blue bottle.', 'A photo of a brown bottle.', 'A photo of a gray bottle.', 'A photo of a green bottle.', 'A photo of a orange bottle.', 'A photo of a red bottle.', 'A photo of a white bottle.', 'A photo of a yellow bottle.', 'A photo of a black cup.', 'A photo of a blue cup.', 'A photo of a brown cup.', 'A photo of a red cup.', 'A photo of a white cup.', 'A photo of a gray fork.', 'A photo of a black knife.', 'A photo of a gray knife.', 'A photo of a gray spoon.', 'A photo of a black bowl.', 'A photo of a blue bowl.', 'A photo of a brown bowl.', 'A photo of a gray bowl.', 'A photo of a red bowl.', 'A photo of a white bowl.', 'A photo of a yellow bowl.', 'A photo of a black banana.', 'A photo of a green banana.', 'A photo of a yellow banana.', 'A photo of a green apple.', 'A photo of a red apple.', 'A photo of a yellow apple.', 'A photo of a brown sandwich.', 'A photo of a tan sandwich.', 'A photo of a white sandwich.', 'A photo of a yellow sandwich.', 'A photo of a orange orange.', 'A photo of a yellow orange.', 'A photo of a green broccoli.', 'A photo of a orange carrot.', 'A photo of a brown hot dog.', 'A photo of a red hot dog.', 'A photo of a tan hot dog.', 'A photo of a brown pizza.', 'A photo of a orange pizza.', 'A photo of a red pizza.', 'A photo of a tan pizza.', 'A photo of a white pizza.', 'A photo of a yellow pizza.', 'A photo of a brown donut.', 'A photo of a gray donut.', 'A photo of a tan donut.', 'A photo of a white donut.', 'A photo of a brown cake.', 'A photo of a red cake.', 'A photo of a tan cake.', 'A photo of a white cake.', 'A photo of a yellow cake.', 'A photo of a black chair.', 'A photo of a blue chair.', 'A photo of a brown chair.', 'A photo of a gray chair.', 'A photo of a green chair.', 'A photo of a red chair.', 'A photo of a tan chair.', 'A photo of a white chair.', 'A photo of a black couch.', 'A photo of a brown couch.', 'A photo of a gray couch.', 'A photo of a red couch.', 'A photo of a tan couch.', 'A photo of a white couch.', 'A photo of a black potted plant.', 'A photo of a brown potted plant.', 'A photo of a green potted plant.', 'A photo of a red potted plant.', 'A photo of a white potted plant.', 'A photo of a yellow potted plant.', 'A photo of a brown bed.', 'A photo of a white bed.', 'A photo of a black dining table.', 'A photo of a brown dining table.', 'A photo of a gray dining table.', 'A photo of a tan dining table.', 'A photo of a white dining table.', 'A photo of a white toilet.', 'A photo of a black tv.', 'A photo of a gray tv.', 'A photo of a white tv.', 'A photo of a black laptop.', 'A photo of a gray laptop.', 'A photo of a white laptop.', 'A photo of a black mouse.', 'A photo of a gray mouse.', 'A photo of a white mouse.', 'A photo of a black remote.', 'A photo of a gray remote.', 'A photo of a white remote.', 'A photo of a black keyboard.', 'A photo of a gray keyboard.', 'A photo of a white keyboard.', 'A photo of a black cell phone.', 'A photo of a gray cell phone.', 'A photo of a white cell phone.', 'A photo of a black microwave.', 'A photo of a white microwave.', 'A photo of a black oven.', 'A photo of a gray oven.', 'A photo of a white oven.', 'A photo of a gray sink.', 'A photo of a white sink.', 'A photo of a gray refrigerator.', 'A photo of a white refrigerator.', 'A photo of a black book.', 'A photo of a blue book.', 'A photo of a brown book.', 'A photo of a gray book.', 'A photo of a red book.', 'A photo of a tan book.', 'A photo of a white book.', 'A photo of a yellow book.', 'A photo of a black clock.', 'A photo of a gray clock.', 'A photo of a white clock.', 'A photo of a black vase.', 'A photo of a brown vase.', 'A photo of a red vase.', 'A photo of a black teddy bear.', 'A photo of a brown teddy bear.', 'A photo of a red teddy bear.', 'A photo of a tan teddy bear.', 'A photo of a white teddy bear.', 'A photo of a white toothbrush.', 'A photo of a metal boat.', 'A photo of a metal stop sign.', 'A photo of a metal bench.', 'A photo of a wooden bench.', 'A photo of a leather handbag.', 'A photo of a plastic skis.', 'A photo of a wooden skis.', 'A photo of a plastic snowboard.', 'A photo of a leather sports ball.', 'A photo of a plastic sports ball.', 'A photo of a wooden baseball bat.', 'A photo of a wooden skateboard.', 'A photo of a plastic surfboard.', 'A photo of a wooden surfboard.', 'A photo of a glass bottle.', 'A photo of a plastic bottle.', 'A photo of a ceramic cup.', 'A photo of a glass cup.', 'A photo of a plastic cup.', 'A photo of a metal fork.', 'A photo of a metal knife.', 'A photo of a metal spoon.', 'A photo of a ceramic bowl.', 'A photo of a glass bowl.', 'A photo of a metal bowl.', 'A photo of a plastic bowl.', 'A photo of a leather chair.', 'A photo of a metal chair.', 'A photo of a plastic chair.', 'A photo of a wooden chair.', 'A photo of a metal dining table.', 'A photo of a plastic dining table.', 'A photo of a wooden dining table.', 'A photo of a ceramic toilet.', 'A photo of a porcelain sink.', 'A photo of a metal sink.', 'A photo of a paper book.', 'A photo of a metal clock.', 'A photo of a ceramic vase.', 'A photo of a glass vase.', 'A photo of a metal scissors.', 'A photo of a plastic toothbrush.', 'A photo of a striped cat.', 'A photo of a striped zebra.', 'A photo of a spotted giraffe.', 'A photo of a striped umbrella.', 'A photo of a tiled umbrella.', 'A photo of a dotted tie.', 'A photo of a striped tie.', 'A photo of a striped kite.']\n"
     ]
    }
   ],
   "source": [
    "# Parse info into separate lists, including \"queries\" with prompts \n",
    "queries = []\n",
    "attr_ids = []\n",
    "obj_ids = []\n",
    "attr_names = []\n",
    "obj_names = []\n",
    "for combo in attr_and_objs_to_retrieve:\n",
    "    queries.append(PROMPT_TEMPLATE.format(attribute_name=combo[2], category_name=combo[3]))\n",
    "    attr_ids.append(combo[0])\n",
    "    obj_ids.append(combo[1])\n",
    "    attr_names.append(combo[2])\n",
    "    obj_names.append(combo[3])\n",
    "print('Queries')\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a9976",
   "metadata": {},
   "source": [
    "### Preprocess images into crops for each region to retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03290d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n"
     ]
    }
   ],
   "source": [
    "if LOAD_IMG_INFO: \n",
    "    \n",
    "    # Just load files\n",
    "    with open(WORK_AREA_PATH + 'test_imgs.pkl', 'rb') as f:\n",
    "        test_imgs = pickle.load(f)\n",
    "    with open(WORK_AREA_PATH + 'test_attr_vectors.pkl', 'rb') as f:\n",
    "        test_attr_vectors = pickle.load(f)\n",
    "    with open(WORK_AREA_PATH + 'test_annot_ids.pkl', 'rb') as f:\n",
    "        test_annot_ids = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    \n",
    "    # Get regions as images, corresponding attribute annotations, and annot cat (obj) ids \n",
    "    test_imgs = []\n",
    "    test_attr_vectors = []\n",
    "    test_annot_ids = []\n",
    "\n",
    "    # Loop through all OVAD annotations\n",
    "    for i, annot in enumerate(ovad_data[\"annotations\"]):\n",
    "\n",
    "        # Get relevant info \n",
    "        img_name = ovad_data['annotations'][i]['image_id']\n",
    "        bbox_to_use = ovad_data['annotations'][i]['bbox']\n",
    "        full_img_path = PATH_TO_COCO + f\"{img_name:012d}\" + '.jpg'\n",
    "        orig_image = Image.open(full_img_path).convert(\"RGB\")\n",
    "\n",
    "        # We use crops; could ROIpool features too (in future)\n",
    "        test_img = orig_image.crop((bbox_to_use[0], bbox_to_use[1], bbox_to_use[0]+bbox_to_use[2], bbox_to_use[1]+bbox_to_use[3]))\n",
    "\n",
    "        # Append information \n",
    "        test_imgs.append(test_img)\n",
    "        test_attr_vectors.append(ovad_data['annotations'][i]['att_vec'])\n",
    "        test_annot_ids.append(ovad_data['annotations'][i]['category_id'])\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "\n",
    "    with open(WORK_AREA_PATH + 'test_imgs.pkl', 'wb') as f:\n",
    "        pickle.dump(test_imgs, f)\n",
    "    with open(WORK_AREA_PATH + 'test_attr_vectors.pkl', 'wb') as f:\n",
    "        pickle.dump(test_attr_vectors, f)\n",
    "    with open(WORK_AREA_PATH + 'test_annot_ids.pkl', 'wb') as f:\n",
    "        pickle.dump(test_annot_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61eb2a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CLIP Wrapper\n",
      "Text features size: \n",
      "torch.Size([323, 512])\n"
     ]
    }
   ],
   "source": [
    "# Get text classifier \n",
    "with torch.no_grad():\n",
    "    text = clip.tokenize(queries).to(DEVICE)\n",
    "    if MODE_FOR_MODEL == \"custom_trained\" or MODE_FOR_MODEL == \"default_openai\":\n",
    "        print('Using CLIP Wrapper')\n",
    "        text_features = model.model.encode_text(text)\n",
    "    else: \n",
    "        text_features = model.encode_text(text)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "print('Text features size: ')\n",
    "print(text_features.shape) # number of queries x text feat dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf61d93",
   "metadata": {},
   "source": [
    "## Inference with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b70cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have image list - go through; encode with CLIP; compute similarities; store similarities in dictionary \n",
    "# We compute similarities for all queries due to ease of access \n",
    "dict_img_to_clip_logits = dict()\n",
    "with torch.no_grad():\n",
    "    for i, ex_img in enumerate(test_imgs): \n",
    "        ex_img = preprocess(ex_img).unsqueeze(0).to(DEVICE)\n",
    "        if MODE_FOR_MODEL == \"custom_trained\" or MODE_FOR_MODEL == \"default_openai\":\n",
    "            image_features = model.model.encode_image(ex_img)\n",
    "        else:\n",
    "            image_features = model.encode_image(ex_img)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * image_features @ text_features.T)\n",
    "        dict_img_to_clip_logits[i] = similarity # 1 x 323\n",
    "        if i % 500 == 0:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa549132",
   "metadata": {},
   "source": [
    "## Aggregate Results From Similarity Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fbc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get precision/recall values to evaluate \n",
    "\n",
    "# Set up empty dictionary for precision/recall\n",
    "overall_precision_values = dict()\n",
    "overall_precision_values[\"top1\"] = []\n",
    "overall_precision_values[\"top5\"] = []\n",
    "overall_precision_values[\"top10\"] = []\n",
    "for attr_type in ATTRIBUTE_TYPES_OF_INTEREST:\n",
    "    overall_precision_values[attr_type] = dict()\n",
    "    overall_precision_values[attr_type][\"top1\"] = []\n",
    "    overall_precision_values[attr_type][\"top5\"] = []\n",
    "    overall_precision_values[attr_type][\"top10\"] = []\n",
    "overall_recall_values = dict()\n",
    "overall_recall_values[\"top1\"] = []\n",
    "overall_recall_values[\"top5\"] = []\n",
    "overall_recall_values[\"top10\"] = []\n",
    "for attr_type in ATTRIBUTE_TYPES_OF_INTEREST:\n",
    "    overall_recall_values[attr_type] = dict()\n",
    "    overall_recall_values[attr_type][\"top1\"] = []\n",
    "    overall_recall_values[attr_type][\"top5\"] = []\n",
    "    overall_recall_values[attr_type][\"top10\"] = []\n",
    "\n",
    "# For each text query (category) \n",
    "for i, c in enumerate(queries): # Category is in form of prompt \n",
    "    print('Query #' + str(i) + ': ' + c)\n",
    "\n",
    "    # Get attribute/object name along with attribute index \n",
    "    attr_index = int(attr_ids[i])\n",
    "    category_index = int(obj_ids[i])\n",
    "    attr_name = attr_names[i]\n",
    "    category_name = obj_names[i]\n",
    "    \n",
    "    # For each image in dictionary get the score for that particular prompt \n",
    "    scores = []\n",
    "    for img_in_dict in dict_img_to_clip_logits:\n",
    "        scores.append(dict_img_to_clip_logits[img_in_dict][0][i].item())\n",
    "        \n",
    "    # For each value of k, get top scores; note if query is satisfied\n",
    "    for k in [1, 5, 10]:\n",
    "        \n",
    "        # For each top index, check if the attribute corresponding to the prompt is marked and if the correct COCO category is picked\n",
    "        top_values, top_indices = torch.topk(torch.Tensor(scores), k)\n",
    "        precision_vals_at_k = []\n",
    "        for ind in top_indices:\n",
    "            # Both obj and attribute should match \n",
    "            if test_attr_vectors[ind][attr_index] == 1 and test_annot_ids[ind] == category_index:\n",
    "                precision_vals_at_k.append(1)\n",
    "            else:\n",
    "                precision_vals_at_k.append(0)\n",
    "        prec_at_k = sum(precision_vals_at_k)/len(precision_vals_at_k)\n",
    "        rec_at_k = 1 if sum(precision_vals_at_k) > 0 else 0            # do we have at least one correct entry?\n",
    "        \n",
    "        # Mark precision at k for each case \n",
    "        overall_precision_values[\"top\" + str(k)].append(prec_at_k)\n",
    "        if attr_index in COLOR_IDS:\n",
    "            overall_precision_values[\"color\"][\"top\" + str(k)].append(prec_at_k)\n",
    "        elif attr_index in PATTERN_IDS:\n",
    "            overall_precision_values[\"pattern\"][\"top\" + str(k)].append(prec_at_k)\n",
    "        elif attr_index in MATERIAL_IDS:\n",
    "            overall_precision_values[\"material\"][\"top\" + str(k)].append(prec_at_k)\n",
    "            \n",
    "        # Mark recall at k for each case\n",
    "        overall_recall_values[\"top\" + str(k)].append(rec_at_k)\n",
    "        if attr_index in COLOR_IDS:\n",
    "            overall_recall_values[\"color\"][\"top\" + str(k)].append(rec_at_k)\n",
    "        elif attr_index in PATTERN_IDS:\n",
    "            overall_recall_values[\"pattern\"][\"top\" + str(k)].append(rec_at_k)\n",
    "        elif attr_index in MATERIAL_IDS:\n",
    "            overall_recall_values[\"material\"][\"top\" + str(k)].append(rec_at_k)\n",
    "\n",
    "    print('\\tCorrect @ ' + str(k) + ': ' + str(precision_vals_at_k))\n",
    "    print('\\tTop Image Indices: ' + str(top_indices.tolist()))\n",
    "    print('\\tSims: ' + str([float(\"{:.2f}\".format(num)) for num in top_values]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54861055",
   "metadata": {},
   "source": [
    "### Print Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57df328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision (i.e. what is the acc of our retrievals?)\")\n",
    "print('overall')\n",
    "print(\"\\t@1: {:.2%}\".format(sum(overall_precision_values[\"top1\"])/len(overall_precision_values[\"top1\"])))\n",
    "print(\"\\t@5: {:.2%}\".format(sum(overall_precision_values[\"top5\"])/len(overall_precision_values[\"top5\"])))\n",
    "print(\"\\t@10: {:.2%}\".format(sum(overall_precision_values[\"top10\"])/len(overall_precision_values[\"top10\"])))\n",
    "print(\"\\t\" + str(len(overall_precision_values[\"top1\"])) + ' samples')\n",
    "for attr_type in ATTRIBUTE_TYPES_OF_INTEREST:\n",
    "    print(attr_type)\n",
    "    print(\"\\t@1: {:.2%}\".format(sum(overall_precision_values[attr_type][\"top1\"])/len(overall_precision_values[attr_type][\"top1\"])))\n",
    "    print(\"\\t@5: {:.2%}\".format(sum(overall_precision_values[attr_type][\"top5\"])/len(overall_precision_values[attr_type][\"top5\"])))\n",
    "    print(\"\\t@10: {:.2%}\".format(sum(overall_precision_values[attr_type][\"top10\"])/len(overall_precision_values[attr_type][\"top10\"])))\n",
    "    print(\"\\t\" + str(len(overall_precision_values[attr_type][\"top1\"])) + ' samples')\n",
    "print()\n",
    "\n",
    "print(\"Recall (i.e. have we found at least 1 sample with correct attr+obj combo?)\")\n",
    "print('overall')\n",
    "print(\"\\t@1: {:.2%}\".format(sum(overall_recall_values[\"top1\"])/len(overall_recall_values[\"top1\"])))\n",
    "print(\"\\t@5: {:.2%}\".format(sum(overall_recall_values[\"top5\"])/len(overall_recall_values[\"top5\"])))\n",
    "print(\"\\t@10: {:.2%}\".format(sum(overall_recall_values[\"top10\"])/len(overall_recall_values[\"top10\"])))\n",
    "print(\"\\t\" + str(len(overall_recall_values[\"top1\"])) + ' samples')\n",
    "for attr_type in ATTRIBUTE_TYPES_OF_INTEREST:\n",
    "    print(attr_type)\n",
    "    print(\"\\t@1: {:.2%}\".format(sum(overall_recall_values[attr_type][\"top1\"])/len(overall_recall_values[attr_type][\"top1\"])))\n",
    "    print(\"\\t@5: {:.2%}\".format(sum(overall_recall_values[attr_type][\"top5\"])/len(overall_recall_values[attr_type][\"top5\"])))\n",
    "    print(\"\\t@10: {:.2%}\".format(sum(overall_recall_values[attr_type][\"top10\"])/len(overall_recall_values[attr_type][\"top10\"])))\n",
    "    print(\"\\t\" + str(len(overall_recall_values[attr_type][\"top1\"])) + ' samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d25da2",
   "metadata": {},
   "source": [
    "## Extra: Visualization to see success/error cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ffb572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imshow function helper \n",
    "def imshow(image, bbox, ax=None, title=None, normalize=True):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    if normalize:\n",
    "        mean = np.array([0.5, 0.5, 0.5])\n",
    "        std = np.array([0.5, 0.5, 0.5])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "    ax.add_patch(Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3],  color='g', linewidth=4, fill=False))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14474cd9",
   "metadata": {},
   "source": [
    "### Visualize specific image id(s) retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b11cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_IMAGE_IDS = [6000, 3655, 6715, 10133, 6130, 5823, 2994, 6971, 6718, 6966]\n",
    "\n",
    "for idx in LIST_OF_IMAGE_IDS:\n",
    "    img_name = ovad_data['annotations'][idx]['image_id']\n",
    "    bbox_to_use = ovad_data['annotations'][idx]['bbox']\n",
    "    full_img_path = PATH_TO_COCO + f\"{img_name:012d}\" + '.jpg'\n",
    "    orig_image = Image.open(full_img_path).convert(\"RGB\")\n",
    "    left = bbox_to_use[0]\n",
    "    upper = bbox_to_use[1]\n",
    "    right = left + bbox_to_use[2]\n",
    "    lower = upper + bbox_to_use[3]\n",
    "    test_img = orig_image.crop((left, upper, right, lower))\n",
    "    imshow(orig_image, bbox_to_use, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef923492",
   "metadata": {},
   "source": [
    "### Visualize all GT examples for specific query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTR_INDEX = 25         # \"blue\"\n",
    "COCO_INDEX = 28         # \"umbrella\"\n",
    "\n",
    "for i, annot in enumerate(ovad_data[\"annotations\"]):\n",
    "    if annot['att_vec'][ATTR_INDEX] == 1 and annot['category_id'] == COCO_INDEX:\n",
    "        img_name = annot['image_id']\n",
    "        bbox_to_use = annot['bbox']\n",
    "        full_img_path = PATH_TO_COCO + f\"{img_name:012d}\" + '.jpg'\n",
    "        orig_image = Image.open(full_img_path).convert(\"RGB\")\n",
    "        test_img = orig_image.crop((bbox_to_use[0], bbox_to_use[1], bbox_to_use[0]+bbox_to_use[2], bbox_to_use[1]+bbox_to_use[3]))\n",
    "        imshow(orig_image, bbox_to_use, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced15bc",
   "metadata": {},
   "source": [
    "### Run classification to compare queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f1313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 12253 #1400 # red car = 11155; 12465; yellow car 6966; brown bird 12253\n",
    "\n",
    "# Plot image example for debug \n",
    "img_name = ovad_data['annotations'][IDX]['image_id']\n",
    "bbox_to_use = ovad_data['annotations'][IDX]['bbox']\n",
    "root_path = '/archive2/kyle/datasets/coco/val2017/'\n",
    "full_img_path = root_path + f\"{img_name:012d}\" + '.jpg'\n",
    "orig_image = Image.open(full_img_path).convert(\"RGB\")\n",
    "left = bbox_to_use[0]\n",
    "upper = bbox_to_use[1]\n",
    "right = left + bbox_to_use[2]\n",
    "lower = upper + bbox_to_use[3]\n",
    "test_img = orig_image.crop((left, upper, right, lower))\n",
    "imshow(orig_image, bbox_to_use, normalize=False)\n",
    "\n",
    "ex_img = preprocess(test_img).unsqueeze(0).to(DEVICE)\n",
    "if MODE_FOR_MODEL == \"custom_trained\" or MODE_FOR_MODEL == \"default_openai\":\n",
    "    image_features = model.model.encode_image(ex_img)\n",
    "else:\n",
    "    image_features = model.encode_image(ex_img)\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "values, indices = similarity[0].topk(5)\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{queries[index]:>16s}: {100 * value.item():.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
